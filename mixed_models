#simulate the data 
import numpy as np
import matplotlib.pyplot as plt

lam = [0.5,1.5]
np.random.seed(1234)
p1 = np.random.poisson(lam[0], 10000)
p2 = np.random.poisson(lam[1], 10000)
model_data = p1 + p2

count, bins, ignored = plt.hist(p1, 50, density=True)
plt.show()
count, bins, ignored = plt.hist(p2, 50, density=True)
plt.show()
count, bins, ignored = plt.hist(model_data, 50, density=True)
plt.show()

#compute likelihoods 
def compute_log_lik(L, w):
    L[:, 0] = L[:, 0] * w[0]
    L[:, 1] = L[:, 1] * w[1]
    return np.sum(np.log(np.sum(L, axis=1)))

#compute the likelihood of mixture comonents currently specified and store the values in L
L = np.empty((p1.shape[0], 2))
L[:, 0] = p1
L[:, 1] = p2

def mixture_EM(w_init, L):
    w_curr = w_init

    # Store log-likelihoods for each iteration
    log_liks = []
    ll = compute_log_lik(L, w_curr)
    log_liks.append(ll)
    delta_ll = 1

    while delta_ll > 1e-5:
        w_curr = EM_iter(w_curr, L)
        ll = compute_log_lik(L, w_curr)
        log_liks.append(ll)
        delta_ll = log_liks[-1] - log_liks[-2]

    return w_curr, log_liks


def EM_iter(w_curr, L):
    # E-step: compute posterior probabilities
    posterior = L.copy()
    for i in range(L.shape[1]):
        posterior[:, i] = w_curr[i] * posterior[:, i]
    posterior = posterior / np.sum(posterior, axis=1)[:, np.newaxis]

    # M-step
    w_next = np.sum(posterior, axis=0) / np.sum(posterior)

    return w_next


#perform EM
ee = mixture_EM([0.5, 0.5], L)
print("Estimate = (", round(ee[0][0], 2), ",", round(ee[0][1], 2), ")", sep="")

RuntimeWarning: divide by zero encountered in log
np.seterr(divide = 'ignore')
np.seterr(divide = 'warn')


